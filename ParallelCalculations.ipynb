{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNCr65RxVzjw2pkxMpsHyUB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnnAnsas/Pet-Project/blob/main/ParallelCalculations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iWruwfV2jJ2X",
        "outputId": "a1e5f3e2-cced-4988-afa8-b21da94b7876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1m_Xqc7-tfaOJ5z-O9eVwELn8oNPCTqnf\n",
            "To: /content/lstm_cuda_jit_fixed_v2.zip\n",
            "100% 6.91k/6.91k [00:00<00:00, 20.5MB/s]\n",
            "Archive:  lstm_cuda_jit_fixed_v2.zip\n",
            " extracting: lstm_cuda_jit_fixed/lstm_cuda.cu  \n",
            " extracting: lstm_cuda_jit_fixed/lstm.cpp  \n",
            " extracting: lstm_cuda_jit_fixed/jit_loader.py  \n",
            " extracting: lstm_cuda_jit_fixed/model.py  \n",
            " extracting: lstm_cuda_jit_fixed/train.py  \n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (1.11.1.4)\n",
            "/content/lstm_cuda_jit_fixed\n"
          ]
        }
      ],
      "source": [
        "#!pip install -U gdown\n",
        "!gdown --id 1m_Xqc7-tfaOJ5z-O9eVwELn8oNPCTqnf --output lstm_cuda_jit_fixed_v2.zip\n",
        "!unzip -o lstm_cuda_jit_fixed_v2.zip\n",
        "!pip install ninja\n",
        "\n",
        "%cd lstm_cuda_jit_fixed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TORCH_CUDA_ARCH_LIST'] = \"7.5\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "ycP-hv-4j2DD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /root/.cache/torch_extensions\n",
        "\n",
        "from jit_loader import extension\n",
        "print(\"ğŸ§  Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ CUDA ÑĞ´Ñ€Ğ°:\", dir(extension))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnzxqzDBozVd",
        "outputId": "1a116e71-f537-4cd8-cc31-7ba2ffad14e0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/py311_cu124/lstm_cuda...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py311_cu124/lstm_cuda/build.ninja...\n",
            "Building extension module lstm_cuda...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§  Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ CUDA ÑĞ´Ñ€Ğ°: ['__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'lstm_forward']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading extension module lstm_cuda...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from model import CustomLSTM\n",
        "import torch\n",
        "\n",
        "model = CustomLSTM(vocab_size=128).cuda()\n",
        "x = torch.randint(0, 128, (4, 32)).cuda()\n",
        "out = model(x)\n",
        "print(\"âœ… Output shape:\", out.shape)  # â†’ [4, 32, 128]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEuKTYeGk6W3",
        "outputId": "a6ed3205-205c-4f78-8a31-f9349533709e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Output shape: torch.Size([4, 32, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "hw6IAGc4T7Qq",
        "outputId": "c7b5fe76-8b1f-4a28-a1ed-6bfc84c5bd6e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/content/lstm_cuda_jit_fixed/train.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m                             )\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;32mand\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_graph_capture_health_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_cuda_graph_capture_health_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         ):\n\u001b[0;32m--> 436\u001b[0;31m             \u001b[0mcapturing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_current_stream_capturing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             if capturing and not all(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/graphs.py\u001b[0m in \u001b[0;36mis_current_stream_capturing\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mIf\u001b[0m \u001b[0ma\u001b[0m \u001b[0mCUDA\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0mdoes\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexist\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0mwithout\u001b[0m \u001b[0minitializing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \"\"\"\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_cuda_isCurrentStreamCapturing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "cwd = os.getcwd()\n",
        "\n",
        "def print_tree(startpath, prefix=\"\"):\n",
        "    for i, name in enumerate(sorted(os.listdir(startpath))):\n",
        "        path = os.path.join(startpath, name)\n",
        "        is_last = i == len(os.listdir(startpath)) - 1\n",
        "        branch = \"â””â”€â”€ \" if is_last else \"â”œâ”€â”€ \"\n",
        "        print(prefix + branch + name)\n",
        "        if os.path.isdir(path):\n",
        "            extension = \"    \" if is_last else \"â”‚   \"\n",
        "            print_tree(path, prefix + extension)\n",
        "\n",
        "# ĞŸĞµÑ‡Ğ°Ñ‚Ğ°ĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°\n",
        "print(\"cwd/\")\n",
        "print_tree(cwd)\n"
      ],
      "metadata": {
        "id": "3u_22JQayPW6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01652ae6-fc07-4672-f0e7-c3cf1122898d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cwd/\n",
            "â”œâ”€â”€ .config\n",
            "â”‚   â”œâ”€â”€ .last_opt_in_prompt.yaml\n",
            "â”‚   â”œâ”€â”€ .last_survey_prompt.yaml\n",
            "â”‚   â”œâ”€â”€ .last_update_check.json\n",
            "â”‚   â”œâ”€â”€ active_config\n",
            "â”‚   â”œâ”€â”€ config_sentinel\n",
            "â”‚   â”œâ”€â”€ configurations\n",
            "â”‚   â”‚   â””â”€â”€ config_default\n",
            "â”‚   â”œâ”€â”€ default_configs.db\n",
            "â”‚   â”œâ”€â”€ gce\n",
            "â”‚   â”œâ”€â”€ hidden_gcloud_config_universe_descriptor_data_cache_configs.db\n",
            "â”‚   â””â”€â”€ logs\n",
            "â”‚       â””â”€â”€ 2025.04.24\n",
            "â”‚           â”œâ”€â”€ 18.19.17.922226.log\n",
            "â”‚           â”œâ”€â”€ 18.19.38.522066.log\n",
            "â”‚           â”œâ”€â”€ 18.19.46.929623.log\n",
            "â”‚           â”œâ”€â”€ 18.19.48.089267.log\n",
            "â”‚           â”œâ”€â”€ 18.19.56.709493.log\n",
            "â”‚           â””â”€â”€ 18.19.57.353004.log\n",
            "â”œâ”€â”€ .ipynb_checkpoints\n",
            "â”œâ”€â”€ Trash'\n",
            "â”‚   â”œâ”€â”€ .ipynb_checkpoints\n",
            "â”‚   â””â”€â”€ __pycache__\n",
            "â”‚       â””â”€â”€ jit_loader.cpython-311.pyc\n",
            "â”œâ”€â”€ lstm_cuda_jit_fixed\n",
            "â”‚   â”œâ”€â”€ __pycache__\n",
            "â”‚   â”‚   â”œâ”€â”€ jit_loader.cpython-311.pyc\n",
            "â”‚   â”‚   â””â”€â”€ model.cpython-311.pyc\n",
            "â”‚   â”œâ”€â”€ jit_loader.py\n",
            "â”‚   â”œâ”€â”€ lstm.cpp\n",
            "â”‚   â”œâ”€â”€ lstm_cuda.cu\n",
            "â”‚   â”œâ”€â”€ model.py\n",
            "â”‚   â””â”€â”€ train.py\n",
            "â”œâ”€â”€ lstm_cuda_jit_fixed_v2.zip\n",
            "â””â”€â”€ sample_data\n",
            "    â”œâ”€â”€ README.md\n",
            "    â”œâ”€â”€ anscombe.json\n",
            "    â”œâ”€â”€ california_housing_test.csv\n",
            "    â”œâ”€â”€ california_housing_train.csv\n",
            "    â”œâ”€â”€ mnist_test.csv\n",
            "    â””â”€â”€ mnist_train_small.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»"
      ],
      "metadata": {
        "id": "iJtwiuS-SB06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "SSuHAGI-ULrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LFASjhmrZOIJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}